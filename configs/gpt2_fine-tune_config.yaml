pretrained_name: &global_name "gpt2"
lr: 0.0001
epochs: 50
step_type: "rl"
train_context: 0.
target_types_pred: 0.5
real_types_match: 0.25
adaptation_part: 0.25
sample_temperature: 2.0
maximize_distance: 0.0
sameness_penalty: 0.001
div_factor: 10000
log_dir: "./gpt2-small"
model_version: 10
ner_model_version: 7
# For input data
pretrained_tokenizer: *global_name
max_token_number: 128
overlap: 32
batch_size: 32