pretrained_name: &global_name "gpt2"
lr: 0.0001
epochs: 50
step_type: "classic"
train_context: 0.
target_types_pred: 1.0
real_types_match: 1.0
adaptation_part: 0.25
sample_temperature: 2.0
maximize_distance: 0.0
sameness_penalty: 2.0
sameness_threshold: 0.08
div_factor: 10000
log_dir: "./gpt2-small"
model_version: 10
ner_model_version: 7
# For input data
pretrained_tokenizer: *global_name
max_token_number: 128
overlap: 32
batch_size: 24  # 32