pretrained_name: &global_name "gpt2"
lr: 0.0001
epochs: 50
adaptation_part: 0.25
step_type: "rl"
train_context: 0.
target_types_pred: 2.1
real_types_match: 2.1
types_weights: [0.005988, 0.633999, 0.031310, 0.267605, 0.005428, 0.027095, 0.028574, 0., 0.]  # Null
sample_temperature: 2.0
with_context: True
maximize_distance: 0.0
sameness_penalty: 2.0
sameness_threshold: 0.08
div_factor: 10000
log_dir: "./gpt2-small"
model_version: 10
ner_model_version: 7
# For input data
pretrained_tokenizer: *global_name
max_token_number: 128
overlap: 40
batch_size: 24  # 32