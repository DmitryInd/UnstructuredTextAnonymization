pretrained_name: &global_name "bert-base-uncased" # "bert-base-cased" / "bert-base-uncased"
lr: 0.0001
epochs: 20
adaptation_epochs: 5
div_factor: 10000
log_dir: "./bert-base"
# For input data
pretrained_tokenizer: *global_name
max_token_number: 100
batch_size: 32